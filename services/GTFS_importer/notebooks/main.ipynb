{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea61e564",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "\n",
    "# files = ['https://storage.googleapis.com/storage/v1/b/mdb-latest/o/ca-british-columbia-translink-vancouver-gtfs-1222.zip?alt=media']\n",
    "files = ['stl.zip']\n",
    "# timeseries = [{'start_time': '06:00:00', 'end_time': '08:59:00', 'value': ''}]\n",
    "timeseries = [\n",
    "\t{'start_time': '06:00:00', 'end_time': '08:59:00', 'value': 'am'},\n",
    "\t{'start_time': '12:00:00', 'end_time': '15:59:00', 'value': 'pm'},\n",
    "]\n",
    "params = {'files': files, 'timeseries': timeseries, 'day': 'tuesday', 'dates': []}\n",
    "\n",
    "\n",
    "default = {'scenario': 'test', 'training_folder': '../..', 'params': params}  # Default execution parameters\n",
    "manual, argv = (True, default) if 'ipykernel' in sys.argv[0] else (False, dict(default, **json.loads(sys.argv[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ffb7908",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import gtfs_kit as gtk\n",
    "import numpy as np\n",
    "from quetzal.io.gtfs_reader import importer\n",
    "from quetzal.model import stepmodel\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36b66424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add path to quetzal\n",
    "sys.path.insert(0, '../../../../quetzal/')\n",
    "import os\n",
    "import numba as nb\n",
    "\n",
    "on_lambda = bool(os.environ.get('AWS_EXECUTION_ENV'))\n",
    "num_cores = nb.config.NUMBA_NUM_THREADS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b016852",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario = argv['scenario']\n",
    "training_folder = argv['training_folder']\n",
    "# if local. add the path to the scenario scenarios/<scenario>/\n",
    "if on_lambda:\n",
    "\tinput_folder = os.path.join(training_folder, 'inputs/')\n",
    "\toutput_folder = os.path.join(training_folder, 'outputs/')\n",
    "else:\n",
    "\tinput_folder = f'../scenarios/{scenario}/inputs/'\n",
    "\toutput_folder = f'../scenarios/{scenario}/outputs/'\n",
    "\tnum_cores = 4\n",
    "\n",
    "if not os.path.exists(output_folder):\n",
    "\tos.makedirs(output_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115bd443",
   "metadata": {},
   "source": [
    "# parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4d5f4eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'files': ['stl.zip'],\n",
       " 'timeseries': [{'start_time': '06:00:00',\n",
       "   'end_time': '08:59:00',\n",
       "   'value': 'am'},\n",
       "  {'start_time': '12:00:00', 'end_time': '15:59:00', 'value': 'pm'}],\n",
       " 'day': 'tuesday',\n",
       " 'dates': []}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "argv['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09e7338d",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = argv['params'].get('files', [])\n",
    "dates = argv['params'].get('dates', [])\n",
    "day = argv['params'].get('day', 'monday')\n",
    "timeseries = argv['params'].get('timeseries', [''])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7116d343",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_period = [t['value'] for t in timeseries]\n",
    "periods = [f'#{p}' if p != '' else p for p in temp_period]\n",
    "assert len(periods) == len(set(periods)), 'cannot have 2 periods with the same name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a638a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_ranges = [[t['start_time'], t['end_time']] for t in timeseries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ffbe7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#am', '#pm']\n",
      "[['06:00:00', '08:59:00'], ['12:00:00', '15:59:00']]\n"
     ]
    }
   ],
   "source": [
    "print(periods)\n",
    "print(time_ranges)\n",
    "assert len(periods) == len(time_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c45a578",
   "metadata": {},
   "outputs": [],
   "source": [
    "DAY_DICT = {'monday': 0, 'tuesday': 1, 'wednesday': 2, 'thursday': 3, 'friday': 4, 'saturday': 5, 'sunday': 6}\n",
    "selected_day = DAY_DICT[day]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5962ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add input folder if its not a url but a zip file on disc\n",
    "paths = []\n",
    "for file in files:\n",
    "\tif file.startswith('http'):\n",
    "\t\tpaths.append(file)\n",
    "\telse:\n",
    "\t\tpaths.append(os.path.join(input_folder, file))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f1894b",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "498147d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing ../scenarios/test/inputs/stl.zip\n"
     ]
    }
   ],
   "source": [
    "feeds = []\n",
    "for path in paths:\n",
    "\tprint('Importing {f}'.format(f=path))\n",
    "\tfeeds.append(importer.GtfsImporter(path=path, dist_units='m'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd9a5a6",
   "metadata": {},
   "source": [
    "# 1) filling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "474ee6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning  stl.zip\n",
      "parent_station missing in stops. set to NaN in stl.zip\n"
     ]
    }
   ],
   "source": [
    "# 1) filling missing values\n",
    "for i in range(len(feeds)):\n",
    "\tprint('cleaning ', files[i])\n",
    "\tif 'agency_id' not in feeds[i].agency:\n",
    "\t\tprint(f'set agency_id to agency_name in {files[i]}')\n",
    "\t\tfeeds[i].agency['agency_id'] = feeds[i].agency['agency_name']\n",
    "\n",
    "\tif 'agency_id' not in feeds[i].routes:\n",
    "\t\tprint(f'add agency_id to routes in {files[i]}')\n",
    "\t\tfeeds[i].routes['agency_id'] = feeds[i].agency['agency_id'].values[0]\n",
    "\n",
    "\tif 'pickup_type' not in feeds[i].stop_times:\n",
    "\t\tprint(f'pickup_type missing in stop_times. set to 0 in {files[i]}')\n",
    "\t\tfeeds[i].stop_times['pickup_type'] = 0\n",
    "\n",
    "\tif 'drop_off_type' not in feeds[i].stop_times:\n",
    "\t\tprint(f'drop_off_type missing in stop_times. set to 0 in {files[i]}')\n",
    "\t\tfeeds[i].stop_times['drop_off_type'] = 0\n",
    "\n",
    "\tif 'parent_station' not in feeds[i].stops:\n",
    "\t\tprint(f'parent_station missing in stops. set to NaN in {files[i]}')\n",
    "\t\tfeeds[i].stops['parent_station'] = np.nan\n",
    "\tif 'stop_code' not in feeds[i].stops:\n",
    "\t\tprint(f'stop_code missing in stops. set to NaN in {files[i]}')\n",
    "\t\tfeeds[i].stops['stop_code'] = np.nan\n",
    "\n",
    "\tfeeds[i].stop_times['pickup_type'].fillna(0, inplace=True)\n",
    "\tfeeds[i].stop_times['drop_off_type'].fillna(0, inplace=True)\n",
    "\tfeeds[i].stop_times['arrival_time'] = feeds[i].stop_times['departure_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7036f8ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad384478",
   "metadata": {},
   "source": [
    "# 1) find patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1003ce34",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feed in feeds:\n",
    "\tfeed.group_services()\n",
    "\tfeed.build_stop_clusters(distance_threshold=50)\n",
    "\tfeed.build_patterns(on='cluster_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e936b6ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b7e9b6e",
   "metadata": {},
   "source": [
    "# get dates if day is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "95dd4b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if dates is not provided as inputs. (we have a day)\n",
    "# get it from first dates of each GTFS\n",
    "if len(dates) == 0:\n",
    "\tfor feed in feeds:\n",
    "\t\ttry:\n",
    "\t\t\tmin_date = feed.calendar['start_date'].unique().min()\n",
    "\t\t\tmax_date = feed.calendar['end_date'].unique().max()\n",
    "\t\texcept:\n",
    "\t\t\tmin_date = feed.calendar_dates['date'].unique().min()\n",
    "\t\t\tmax_date = feed.calendar_dates['date'].unique().max()\n",
    "\n",
    "\t\t# get date range\n",
    "\t\ts = pd.date_range(min_date, max_date, freq='D').to_series()\n",
    "\t\ttry:\n",
    "\t\t\t# get dayofweek selected and take first one\n",
    "\t\t\ts = s[s.dt.dayofweek == selected_day][0]\n",
    "\t\t\t# format  ex: ['20231011'] and append\n",
    "\t\t\tdates.append(f'{s.year}{str(s.month).zfill(2)}{str(s.day).zfill(2)}')\n",
    "\t\texcept:\n",
    "\t\t\tprint('date not available. use', min_date)\n",
    "\t\t\tdates.append(min_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbc6a5b",
   "metadata": {},
   "source": [
    "# 2) restric feed to date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b1559477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restrict feed\n"
     ]
    }
   ],
   "source": [
    "# 2) restric feed to date\n",
    "feeds_t = []\n",
    "print('restrict feed')\n",
    "for i, feed in enumerate(feeds):\n",
    "\tfeed_t = feed.restrict(dates=[dates[i]])  # keep time range. will restrict later\n",
    "\tif len(feed_t.trips) > 0:\n",
    "\t\tfeeds_t.append(feed_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bfa31ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "del feeds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7294e8fb",
   "metadata": {},
   "source": [
    "# 3) add shape_dist_traveled to shapes and stop_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0fd61379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add shape_dist_traveled to shapes\n",
      "add shape_dist_traveled to stop_times\n"
     ]
    }
   ],
   "source": [
    "print('add shape_dist_traveled to shapes')\n",
    "for feed in feeds_t:\n",
    "\tif feed.shapes is None:\n",
    "\t\tprint('no shapes in gtfs')\n",
    "\t\tcontinue\n",
    "\telif 'shape_dist_traveled' not in feed.shapes.columns:\n",
    "\t\tfeed.append_dist_to_shapes()\n",
    "\telif any(feed.shapes['shape_dist_traveled'].isnull()):\n",
    "\t\tfeed.append_dist_to_shapes()\n",
    "\n",
    "print('add shape_dist_traveled to stop_times')\n",
    "for feed in feeds_t:\n",
    "\tif feed.shapes is None:\n",
    "\t\tprint('no shapes in gtfs cannot add to stop_times')\n",
    "\t\tcontinue\n",
    "\telif 'shape_dist_traveled' not in feed.stop_times.columns:\n",
    "\t\tfeed.append_dist_to_stop_times_fast()\n",
    "\telse:\n",
    "\t\tnan_sequence = feed.stop_times[feed.stop_times['shape_dist_traveled'].isnull()]['stop_sequence'].unique()\n",
    "\t\t# if there but all nan are at seq=1. just fill wwith 0.\n",
    "\t\tif all(seq == 1 for seq in nan_sequence):\n",
    "\t\t\tfeed.stop_times['shape_dist_traveled'] = feed.stop_times['shape_dist_traveled'].fillna(0)\n",
    "\t\telse:\n",
    "\t\t\tfeed.append_dist_to_stop_times_fast()\n",
    "\n",
    "\tif feed.stop_times['shape_dist_traveled'].max() < 100:\n",
    "\t\tprint(f'convert to meters')\n",
    "\t\tfeed.dist_units = 'km'\n",
    "\t\tfeed = gtk.convert_dist(feed, new_dist_units='m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aa8b1c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nothing to export. export empty geojson\n",
    "if len(feeds_t) == 0:\n",
    "\tlinks = gpd.GeoDataFrame(columns=['feature'], geometry='feature', crs=4326)\n",
    "\tnodes = gpd.GeoDataFrame(columns=['feature'], geometry='feature', crs=4326)\n",
    "\tlinks.to_file(os.path.join(output_folder, 'links.geojson'), driver='GeoJSON')\n",
    "\tnodes.to_file(os.path.join(output_folder, 'nodes.geojson'), driver='GeoJSON')\n",
    "\tend_of_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ded33d",
   "metadata": {},
   "source": [
    "# 4) build links and nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2c8160b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building links and nodes  stl.zip #am\n",
      "Building links and nodes  stl.zip #pm\n"
     ]
    }
   ],
   "source": [
    "links_concat = []\n",
    "nodes_concat = []\n",
    "for p in range(len(time_ranges)):\n",
    "\ttime_range = time_ranges[p]\n",
    "\tlinks = pd.DataFrame()\n",
    "\tnodes = pd.DataFrame()\n",
    "\tfor i in range(len(feeds_t)):\n",
    "\t\tprint('Building links and nodes ', files[i], periods[p])\n",
    "\t\tfeed = feeds_t[i].copy()\n",
    "\t\tfeed_frequencies = feed.convert_to_frequencies(time_range=time_range)\n",
    "\t\tshapes = feed_frequencies.shapes is not None\n",
    "\t\tfeed_frequencies.build_links_and_nodes(\n",
    "\t\t\tlog=False,\n",
    "\t\t\tshape_dist_traveled=shapes,\n",
    "\t\t\tfrom_shape=shapes,\n",
    "\t\t\tstick_nodes_on_links=shapes,\n",
    "\t\t\tkeep_origin_columns=['departure_time', 'pickup_type'],\n",
    "\t\t\tkeep_destination_columns=['arrival_time', 'drop_off_type'],\n",
    "\t\t\tnum_cores=num_cores,\n",
    "\t\t)\n",
    "\t\tlinks = pd.concat([links, feed_frequencies.links.to_crs(4326)])\n",
    "\t\tnodes = pd.concat([nodes, feed_frequencies.nodes.to_crs(4326)])\n",
    "\tlinks_concat.append(links)\n",
    "\tnodes_concat.append(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c0b89e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = pd.concat(nodes_concat)[['stop_id', 'stop_name', 'stop_code', 'geometry']]\n",
    "nodes = nodes.drop_duplicates(subset=['stop_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270758f0",
   "metadata": {},
   "source": [
    "# clean columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cc49c790",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "\t'trip_id',\n",
    "\t'route_id',\n",
    "\t'agency_id',\n",
    "\t'direction_id',\n",
    "\t'a',\n",
    "\t'b',\n",
    "\t'shape_dist_traveled',\n",
    "\t'link_sequence',\n",
    "\t'time',\n",
    "\t'headway',\n",
    "\t'pickup_type',\n",
    "\t'drop_off_type',\n",
    "\t'route_short_name',\n",
    "\t'route_type',\n",
    "\t'route_color',\n",
    "\t'geometry',\n",
    "]\n",
    "\n",
    "for i in range(len(links_concat)):\n",
    "\tfor col in columns:\n",
    "\t\tif col not in links_concat[i].columns:\n",
    "\t\t\tlinks_concat[i][col] = np.nan\n",
    "\tlinks_concat[i] = links_concat[i][columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "26db3f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change time = 0 to time = 1 so we dont get infinit speed.\n",
    "for i in range(len(links_concat)):\n",
    "\tlinks_concat[i].loc[links_concat[i]['time'] == 0, 'time'] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9baaea6",
   "metadata": {},
   "source": [
    "# 5) concat per period (add #period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1f88f886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename time and headway with period (time#am, time#pm)\n",
    "periods_cols = ['time', 'headway']\n",
    "for i, p in enumerate(periods):\n",
    "\tperiod_dict = {col: f'{col}{p}' for col in periods_cols}\n",
    "\tlinks_concat[i] = links_concat[i].rename(columns=period_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7917850f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_on = ['trip_id', 'a', 'b']\n",
    "links = links_concat[0]\n",
    "\n",
    "for new_links in links_concat[1:]:\n",
    "\tlinks = links.merge(new_links, left_on=merge_on, right_on=merge_on, how='outer', suffixes=('', '_new'))\n",
    "\n",
    "\tfor col in columns:\n",
    "\t\tif col not in merge_on and col in new_links.columns:\n",
    "\t\t\tlinks[col] = links[col].combine_first(links[f'{col}_new'])\n",
    "\n",
    "\tlinks = links.drop(columns=[c for c in links.columns if c.endswith('_new')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bd9bbe",
   "metadata": {},
   "source": [
    "# fixe time and headway NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "92a9ce25",
   "metadata": {},
   "outputs": [],
   "source": [
    "links[[f'time{p}' for p in periods]] = links[[f'time{p}' for p in periods]].fillna(1)\n",
    "links[[f'headway{p}' for p in periods]] = links[[f'headway{p}' for p in periods]].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8048c5c8",
   "metadata": {},
   "source": [
    "# rename route_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b0290990",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('route_type.json') as file:\n",
    "\tmapping = json.load(file)\n",
    "\tmapping = {int(key): item for key, item in mapping.items()}\n",
    "\n",
    "retire = ['taxi']\n",
    "links['route_type'] = links['route_type'].apply(lambda t: mapping.get(t, np.nan))\n",
    "links = links[~links['route_type'].isin(retire)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ae0050",
   "metadata": {},
   "source": [
    "# finish model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "942f0cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = stepmodel.StepModel(epsg=4326, coordinates_unit='meter')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "00c687f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.links = links\n",
    "sm.nodes = nodes\n",
    "\n",
    "sm.nodes = sm.nodes.reset_index(drop=True).sort_index()\n",
    "sm.links = sm.links.reset_index(drop=True).sort_index()\n",
    "\n",
    "sm.nodes.loc[sm.nodes['stop_code'].isna(), 'stop_code'] = sm.nodes.loc[sm.nodes['stop_code'].isna(), 'stop_id']\n",
    "\n",
    "sm.links['trip_id'] = sm.links['agency_id'].astype(str) + '_' + sm.links['trip_id'].astype(str)\n",
    "sm.links['route_id'] = sm.links['agency_id'].astype(str) + '_' + sm.links['route_id'].astype(str)\n",
    "\n",
    "sm.links = sm.links.sort_values(['route_type', 'trip_id']).reset_index(drop=True)\n",
    "\n",
    "dnodes = ('node_' + sm.nodes.reset_index().set_index('stop_id')['index'].astype(str)).to_dict()\n",
    "sm.nodes.index = 'node_' + sm.nodes.index.astype(str)\n",
    "\n",
    "sm.links.index = 'link_' + sm.links.index.astype(str)\n",
    "\n",
    "sm.links['a'] = sm.links['a'].apply(lambda a: dnodes.get(a))\n",
    "sm.links['b'] = sm.links['b'].apply(lambda a: dnodes.get(a))\n",
    "\n",
    "sm.links.drop_duplicates(subset=['trip_id', 'link_sequence'], inplace=True)\n",
    "\n",
    "# Tag route with only one trip\n",
    "# time_slot = np.diff([hhmmss_to_seconds_since_midnight(time) for time in time_range])[0]\n",
    "# sm.links.loc[(time_slot/sm.links['headway']) < 2.0, 'headway'] = np.nan\n",
    "\n",
    "sm.links = sm.links.to_crs(4326)\n",
    "sm.nodes = sm.nodes.to_crs(4326)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d6baf07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add speed, add length.\n",
    "epsg = importer.get_epsg(sm.nodes.iloc[0]['geometry'].y, sm.nodes.iloc[0]['geometry'].x)\n",
    "sm.links['length'] = sm.links.to_crs(epsg).length\n",
    "for p in periods:\n",
    "\tsm.links[f'speed{p}'] = (sm.links['length'] / sm.links[f'time{p}']) * 3.6\n",
    "# regarder quetzal_transit pour voir les valeurs necessaires."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02120457",
   "metadata": {},
   "source": [
    "# export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b9eaea27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving\n"
     ]
    }
   ],
   "source": [
    "print('Saving')\n",
    "sm.links.to_file(os.path.join(output_folder, 'links.geojson'), driver='GeoJSON')\n",
    "sm.nodes.to_file(os.path.join(output_folder, 'nodes.geojson'), driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417d7435",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc5fee8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee7fcbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quetzal-CjBfBe3R-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
